{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b87dc127",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from transformers import DistilBertForQuestionAnswering\n",
    "from transformers import default_data_collator\n",
    "from transformers import AdamW\n",
    "from tqdm.notebook import tqdm\n",
    "import datetime\n",
    "import numpy as np\n",
    "import collections\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af42dbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36036c912e304db684231723ae80c9c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3436dfcbcf4f10b3173c75b3dbc9d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a846bc443f624808a1f21cb65f912cf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "752d951a5e9a40199d54fb76547367a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1eed4f6804440f0b2ee0e4c3f405c9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdf0e26e295a4dd982a09242db043b01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "013f9305f6e345308d15381337f0d0be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e942335f70c437d8a55f91cac7190ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41678bd0fb324caca2ba622585a94614",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 7500\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "        num_rows: 750\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"squad\")\n",
    "\n",
    "#lets sample a small dataset\n",
    "dataset['train'] = dataset['train'].select([i for i in range(7500)])\n",
    "dataset['validation'] = dataset['validation'].select([i for i in range(750)])\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21e5df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_preprocess(examples):\n",
    "    \n",
    "    \"\"\"\n",
    "    generate start and end indexes of answer in context\n",
    "    \"\"\"\n",
    "    \n",
    "    def find_context_start_end_index(sequence_ids):\n",
    "        \"\"\"\n",
    "        returns the token index in whih context starts and ends\n",
    "        \"\"\"\n",
    "        token_idx = 0\n",
    "        while sequence_ids[token_idx] != 1:  #means its special tokens or tokens of queston\n",
    "            token_idx += 1                   # loop only break when context starts in tokens\n",
    "        context_start_idx = token_idx\n",
    "    \n",
    "        while sequence_ids[token_idx] == 1:\n",
    "            token_idx += 1\n",
    "        context_end_idx = token_idx - 1\n",
    "        return context_start_idx,context_end_idx  \n",
    "    \n",
    "    \n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    context = examples[\"context\"]\n",
    "    answers = examples[\"answers\"]\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        context,\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,  #returns id of base context\n",
    "        return_offsets_mapping=True,  # returns (start_index,end_index) of each token\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    \n",
    "    for i,mapping_idx_pairs in enumerate(inputs['offset_mapping']):\n",
    "        context_idx = inputs['overflow_to_sample_mapping'][i]\n",
    "    \n",
    "        # from main context\n",
    "        answer = answers[context_idx]\n",
    "        answer_start_char_idx = answer['answer_start'][0]\n",
    "        answer_end_char_idx = answer_start_char_idx + len(answer['text'][0])\n",
    "\n",
    "    \n",
    "        # now we have to find it in sub contexts\n",
    "        tokens = inputs['input_ids'][i]\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "   \n",
    "        # finding the context start and end indexes wrt sub context tokens\n",
    "        context_start_idx,context_end_idx = find_context_start_end_index(sequence_ids)\n",
    "    \n",
    "        #if the answer is not fully inside context label it as (0,0)\n",
    "        # starting and end index of charecter of full context text\n",
    "        context_start_char_index = mapping_idx_pairs[context_start_idx][0]\n",
    "        context_end_char_index = mapping_idx_pairs[context_end_idx][1]\n",
    "    \n",
    "\n",
    "        #If the answer is not fully inside the context, label is (0, 0)\n",
    "        if (context_start_char_index > answer_start_char_idx) or (\n",
    "            context_end_char_index < answer_end_char_idx):\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "    \n",
    "        else:\n",
    "\n",
    "            # else its start and end token positions\n",
    "            # here idx indicates index of token\n",
    "            idx = context_start_idx\n",
    "            while idx <= context_end_idx and mapping_idx_pairs[idx][0] <= answer_start_char_idx:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)  \n",
    "        \n",
    "\n",
    "            idx = context_end_idx\n",
    "            while idx >= context_start_idx and mapping_idx_pairs[idx][1] > answer_end_char_idx:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "    \n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs\n",
    "\n",
    "def preprocess_validation_examples(examples):\n",
    "    \"\"\"\n",
    "    preprocessing validation data\n",
    "    \"\"\"\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=512,\n",
    "        truncation=\"only_second\",\n",
    "        stride=128,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    base_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        \n",
    "        # take the base id (ie in cases of overflow happens we get base id)\n",
    "        base_context_idx = sample_map[i]\n",
    "        base_ids.append(examples[\"id\"][base_context_idx])\n",
    "        \n",
    "        # sequence id indicates the input. 0 for first input and 1 for second input\n",
    "        # and None for special tokens by default\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        # for Question tokens provide offset_mapping as None\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"base_id\"] = base_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0ef37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataQA(Dataset):\n",
    "    def __init__(self, dataset,mode=\"train\"):\n",
    "        self.mode = mode\n",
    "        \n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "            # sampling\n",
    "            self.dataset = dataset[\"train\"]\n",
    "            self.data = self.dataset.map(train_data_preprocess,\n",
    "                                                      batched=True,\n",
    "                            remove_columns= dataset[\"train\"].column_names)\n",
    "        \n",
    "        else:\n",
    "            self.dataset = dataset[\"validation\"]\n",
    "            self.data = self.dataset.map(preprocess_validation_examples,\n",
    "            batched=True,remove_columns = dataset[\"validation\"].column_names,\n",
    "               )\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        out = {}\n",
    "        example = self.data[idx]\n",
    "        out['input_ids'] = torch.tensor(example['input_ids'])\n",
    "        out['attention_mask'] = torch.tensor(example['attention_mask'])\n",
    "\n",
    "        \n",
    "        if self.mode == \"train\":\n",
    "\n",
    "            out['start_positions'] = torch.unsqueeze(torch.tensor(example['start_positions']),dim=0)\n",
    "            out['end_positions'] = torch.unsqueeze(torch.tensor(example['end_positions']),dim=0)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c30d8608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d6f927e57540d8959b74cce95c6e63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/750 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids :  torch.Size([512])\n",
      "attention_mask :  torch.Size([512])\n",
      "start_positions :  torch.Size([1])\n",
      "end_positions :  torch.Size([1])\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids :  torch.Size([512])\n",
      "attention_mask :  torch.Size([512])\n",
      "start_positions :  torch.Size([1])\n",
      "end_positions :  torch.Size([1])\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids :  torch.Size([512])\n",
      "attention_mask :  torch.Size([512])\n",
      "start_positions :  torch.Size([1])\n",
      "end_positions :  torch.Size([1])\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids :  torch.Size([512])\n",
      "attention_mask :  torch.Size([512])\n",
      "start_positions :  torch.Size([1])\n",
      "end_positions :  torch.Size([1])\n",
      "--------------------------------------------------------------------------------\n",
      "____________________________________________________________________________________________________\n",
      "input_ids :  512\n",
      "attention_mask :  512\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids :  512\n",
      "attention_mask :  512\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids :  512\n",
      "attention_mask :  512\n",
      "--------------------------------------------------------------------------------\n",
      "input_ids :  512\n",
      "attention_mask :  512\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "trained_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(trained_checkpoint)\n",
    "\n",
    "\n",
    "train_dataset = DataQA(dataset,mode=\"train\")\n",
    "val_dataset = DataQA(dataset,mode=\"validation\")\n",
    "\n",
    "\n",
    "\n",
    "for i,d in enumerate(train_dataset):\n",
    "    for k in d.keys():\n",
    "        print(k + ' : ', d[k].shape)\n",
    "    print('--'*40)\n",
    "\n",
    "    if i == 3:\n",
    "        break\n",
    "        \n",
    "print('__'*50)\n",
    "\n",
    "for i,d in enumerate(val_dataset):\n",
    "    for k in d.keys():\n",
    "        print(k + ' : ', len(d[k]))\n",
    "    print('--'*40)\n",
    "    \n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea85290a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 1])\n",
      "torch.Size([2, 1])\n",
      "------------------------------------------------------------\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    shuffle=True,\n",
    "    collate_fn=default_data_collator,\n",
    "    batch_size=2,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    val_dataset, collate_fn=default_data_collator, batch_size=2\n",
    ")\n",
    "\n",
    "for batch in train_dataloader:\n",
    "   print(batch['input_ids'].shape)\n",
    "   print(batch['attention_mask'].shape)\n",
    "   print(batch['start_positions'].shape)\n",
    "   print(batch['end_positions'].shape)\n",
    "   break\n",
    "\n",
    "print('---'*20)\n",
    "\n",
    "for batch in eval_dataloader:\n",
    "   print(batch['input_ids'].shape)\n",
    "   print(batch['attention_mask'].shape)\n",
    "   break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83b7d7dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available device: cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b288e38232a4a43bf3acf1fb35ad3bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Available device: {device}')\n",
    "\n",
    "checkpoint =  \"distilbert-base-uncased\"\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(checkpoint)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "563fd78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11265\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "epochs = 3\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "# (Note that this is not the same as the number of training samples).\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "print(total_steps)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1a16cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need processed validation data to get offsets at the time of evaluation\n",
    "validation_processed_dataset = dataset[\"validation\"].map(preprocess_validation_examples, batched=True,\n",
    "                                                         remove_columns = dataset[\"validation\"].column_names,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fdae5761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answers_and_evaluate(start_logits,end_logits,eval_set,examples):\n",
    "    \"\"\"\n",
    "    make predictions \n",
    "    Args:\n",
    "    start_logits : strat_position prediction logits\n",
    "    end_logits: end_position prediction logits\n",
    "    eval_set: processed val data\n",
    "    examples: unprocessed val data with context text\n",
    "    \"\"\"\n",
    "    # appending all id's corresponding to the base context id\n",
    "    example_to_features = collections.defaultdict(list)\n",
    "    for idx, feature in enumerate(eval_set):\n",
    "        example_to_features[feature[\"base_id\"]].append(idx)\n",
    "\n",
    "    n_best = 20\n",
    "    max_answer_length = 30\n",
    "    predicted_answers = []\n",
    "\n",
    "    for example in examples:\n",
    "        example_id = example[\"id\"]\n",
    "        context = example[\"context\"]\n",
    "        answers = []\n",
    "\n",
    "        # looping through each sub contexts corresponding to a context and finding\n",
    "        # answers\n",
    "        for feature_index in example_to_features[example_id]:\n",
    "            start_logit = start_logits[feature_index]\n",
    "            end_logit = end_logits[feature_index]\n",
    "            offsets = eval_set[\"offset_mapping\"][feature_index]\n",
    "        \n",
    "            # sorting the predictions of all hidden states and taking best n_best prediction\n",
    "            # means taking the index of top 20 tokens\n",
    "            start_indexes = np.argsort(start_logit).tolist()[::-1][:n_best]\n",
    "            end_indexes = np.argsort(end_logit).tolist()[::-1][:n_best]\n",
    "        \n",
    "    \n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                \n",
    "                    # Skip answers that are not fully in the context\n",
    "                    if offsets[start_index] is None or offsets[end_index] is None:\n",
    "                        continue\n",
    "                    # Skip answers with a length that is either < 0 or > max_answer_length.\n",
    "                    if (\n",
    "                        end_index < start_index\n",
    "                        or end_index - start_index + 1 > max_answer_length\n",
    "                       ):\n",
    "                        continue\n",
    "\n",
    "                    answers.append({\n",
    "                        \"text\": context[offsets[start_index][0] : offsets[end_index][1]],\n",
    "                        \"logit_score\": start_logit[start_index] + end_logit[end_index],\n",
    "                        })\n",
    "\n",
    "    \n",
    "            # Select the answer with the best score\n",
    "        if len(answers) > 0:\n",
    "            best_answer = max(answers, key=lambda x: x[\"logit_score\"])\n",
    "            predicted_answers.append(\n",
    "                {\"id\": example_id, \"prediction_text\": best_answer[\"text\"]}\n",
    "            )\n",
    "        else:\n",
    "            predicted_answers.append({\"id\": example_id, \"prediction_text\": \"\"})\n",
    "    \n",
    "    metric = evaluate.load(\"squad\")\n",
    "\n",
    "    theoretical_answers = [\n",
    "            {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in examples\n",
    "    ]\n",
    "    \n",
    "    metric_ = metric.compute(predictions=predicted_answers, references=theoretical_answers)\n",
    "    return predicted_answers,metric_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d91c5422",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "=====Epoch 1=====\n",
      "Training....\n",
      "  Batch    40  of  3,755.    Elapsed: 0:03:37.\n",
      "  Batch    80  of  3,755.    Elapsed: 0:07:23.\n",
      "  Batch   120  of  3,755.    Elapsed: 0:11:09.\n",
      "  Batch   160  of  3,755.    Elapsed: 0:14:56.\n",
      "  Batch   200  of  3,755.    Elapsed: 0:18:44.\n",
      "  Batch   240  of  3,755.    Elapsed: 0:22:30.\n",
      "  Batch   280  of  3,755.    Elapsed: 0:26:20.\n",
      "  Batch   320  of  3,755.    Elapsed: 0:30:09.\n",
      "  Batch   360  of  3,755.    Elapsed: 0:33:52.\n",
      "  Batch   400  of  3,755.    Elapsed: 0:42:42.\n",
      "  Batch   440  of  3,755.    Elapsed: 0:46:19.\n",
      "  Batch   480  of  3,755.    Elapsed: 0:50:44.\n",
      "  Batch   520  of  3,755.    Elapsed: 7:35:42.\n",
      "  Batch   560  of  3,755.    Elapsed: 7:38:53.\n",
      "  Batch   600  of  3,755.    Elapsed: 7:42:07.\n",
      "  Batch   640  of  3,755.    Elapsed: 7:45:22.\n",
      "  Batch   680  of  3,755.    Elapsed: 7:48:44.\n",
      "  Batch   720  of  3,755.    Elapsed: 7:52:22.\n",
      "  Batch   760  of  3,755.    Elapsed: 7:56:00.\n",
      "  Batch   800  of  3,755.    Elapsed: 7:59:38.\n",
      "  Batch   840  of  3,755.    Elapsed: 8:03:14.\n",
      "  Batch   880  of  3,755.    Elapsed: 8:06:50.\n",
      "  Batch   920  of  3,755.    Elapsed: 8:10:26.\n",
      "  Batch   960  of  3,755.    Elapsed: 8:14:03.\n",
      "  Batch 1,000  of  3,755.    Elapsed: 8:17:37.\n",
      "  Batch 1,040  of  3,755.    Elapsed: 8:21:28.\n",
      "  Batch 1,080  of  3,755.    Elapsed: 8:25:17.\n",
      "  Batch 1,120  of  3,755.    Elapsed: 8:29:08.\n",
      "  Batch 1,160  of  3,755.    Elapsed: 8:33:04.\n",
      "  Batch 1,200  of  3,755.    Elapsed: 8:36:55.\n",
      "  Batch 1,240  of  3,755.    Elapsed: 8:40:39.\n",
      "  Batch 1,280  of  3,755.    Elapsed: 8:44:22.\n",
      "  Batch 1,320  of  3,755.    Elapsed: 8:48:09.\n",
      "  Batch 1,360  of  3,755.    Elapsed: 8:51:55.\n",
      "  Batch 1,400  of  3,755.    Elapsed: 8:55:40.\n",
      "  Batch 1,440  of  3,755.    Elapsed: 8:59:23.\n",
      "  Batch 1,480  of  3,755.    Elapsed: 9:03:00.\n",
      "  Batch 1,520  of  3,755.    Elapsed: 9:06:36.\n",
      "  Batch 1,560  of  3,755.    Elapsed: 9:10:15.\n",
      "  Batch 1,600  of  3,755.    Elapsed: 9:13:58.\n",
      "  Batch 1,640  of  3,755.    Elapsed: 9:17:48.\n",
      "  Batch 1,680  of  3,755.    Elapsed: 9:21:36.\n",
      "  Batch 1,720  of  3,755.    Elapsed: 9:25:53.\n",
      "  Batch 1,760  of  3,755.    Elapsed: 9:28:29.\n",
      "  Batch 1,800  of  3,755.    Elapsed: 9:31:15.\n",
      "  Batch 1,840  of  3,755.    Elapsed: 9:34:03.\n",
      "  Batch 1,880  of  3,755.    Elapsed: 9:36:50.\n",
      "  Batch 1,920  of  3,755.    Elapsed: 9:39:40.\n",
      "  Batch 1,960  of  3,755.    Elapsed: 9:42:33.\n",
      "  Batch 2,000  of  3,755.    Elapsed: 9:45:27.\n",
      "  Batch 2,040  of  3,755.    Elapsed: 9:48:22.\n",
      "  Batch 2,080  of  3,755.    Elapsed: 9:51:15.\n",
      "  Batch 2,120  of  3,755.    Elapsed: 9:54:07.\n",
      "  Batch 2,160  of  3,755.    Elapsed: 9:56:56.\n",
      "  Batch 2,200  of  3,755.    Elapsed: 9:59:45.\n",
      "  Batch 2,240  of  3,755.    Elapsed: 10:02:26.\n",
      "  Batch 2,280  of  3,755.    Elapsed: 10:05:05.\n",
      "  Batch 2,320  of  3,755.    Elapsed: 10:07:41.\n",
      "  Batch 2,360  of  3,755.    Elapsed: 10:10:16.\n",
      "  Batch 2,400  of  3,755.    Elapsed: 10:12:52.\n",
      "  Batch 2,440  of  3,755.    Elapsed: 10:15:28.\n",
      "  Batch 2,480  of  3,755.    Elapsed: 10:17:59.\n",
      "  Batch 2,520  of  3,755.    Elapsed: 10:20:36.\n",
      "  Batch 2,560  of  3,755.    Elapsed: 10:23:14.\n",
      "  Batch 2,600  of  3,755.    Elapsed: 10:25:56.\n",
      "  Batch 2,640  of  3,755.    Elapsed: 10:28:25.\n",
      "  Batch 2,680  of  3,755.    Elapsed: 10:31:12.\n",
      "  Batch 2,720  of  3,755.    Elapsed: 10:33:40.\n",
      "  Batch 2,760  of  3,755.    Elapsed: 10:36:05.\n",
      "  Batch 2,800  of  3,755.    Elapsed: 10:38:22.\n",
      "  Batch 2,840  of  3,755.    Elapsed: 10:40:36.\n",
      "  Batch 2,880  of  3,755.    Elapsed: 10:42:47.\n",
      "  Batch 2,920  of  3,755.    Elapsed: 10:44:56.\n",
      "  Batch 2,960  of  3,755.    Elapsed: 10:47:04.\n",
      "  Batch 3,000  of  3,755.    Elapsed: 10:49:18.\n",
      "  Batch 3,040  of  3,755.    Elapsed: 10:51:32.\n",
      "  Batch 3,080  of  3,755.    Elapsed: 10:53:41.\n",
      "  Batch 3,120  of  3,755.    Elapsed: 10:55:50.\n",
      "  Batch 3,160  of  3,755.    Elapsed: 10:58:01.\n",
      "  Batch 3,200  of  3,755.    Elapsed: 11:00:11.\n",
      "  Batch 3,240  of  3,755.    Elapsed: 11:02:22.\n",
      "  Batch 3,280  of  3,755.    Elapsed: 11:04:34.\n",
      "  Batch 3,320  of  3,755.    Elapsed: 11:06:45.\n",
      "  Batch 3,360  of  3,755.    Elapsed: 11:08:57.\n",
      "  Batch 3,400  of  3,755.    Elapsed: 11:12:05.\n",
      "  Batch 3,440  of  3,755.    Elapsed: 11:15:35.\n",
      "  Batch 3,480  of  3,755.    Elapsed: 11:19:14.\n",
      "  Batch 3,520  of  3,755.    Elapsed: 11:23:03.\n",
      "  Batch 3,560  of  3,755.    Elapsed: 11:26:39.\n",
      "  Batch 3,600  of  3,755.    Elapsed: 11:30:10.\n",
      "  Batch 3,640  of  3,755.    Elapsed: 11:33:44.\n",
      "  Batch 3,680  of  3,755.    Elapsed: 11:37:15.\n",
      "  Batch 3,720  of  3,755.    Elapsed: 11:40:50.\n",
      "\n",
      "  Average training loss: 0.61\n",
      "  Training epoch took: 11:43:54\n",
      "\n",
      "Running Validation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "888b74e59d244e8f94b5a885340dd6b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "039633f058cc4b5d8c1e81b97eef891e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/3.32k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: 25.733333333333334, F1 score: 62.46494776351328\n",
      "\n",
      "  Validation took: 0:40:28\n",
      " \n",
      "=====Epoch 2=====\n",
      "Training....\n",
      "  Batch    40  of  3,755.    Elapsed: 0:03:48.\n",
      "  Batch    80  of  3,755.    Elapsed: 0:07:45.\n",
      "  Batch   120  of  3,755.    Elapsed: 0:11:42.\n",
      "  Batch   160  of  3,755.    Elapsed: 0:15:39.\n",
      "  Batch   200  of  3,755.    Elapsed: 0:19:36.\n",
      "  Batch   240  of  3,755.    Elapsed: 0:23:30.\n",
      "  Batch   280  of  3,755.    Elapsed: 0:27:23.\n",
      "  Batch   320  of  3,755.    Elapsed: 0:31:07.\n",
      "  Batch   360  of  3,755.    Elapsed: 0:34:51.\n",
      "  Batch   400  of  3,755.    Elapsed: 0:38:42.\n",
      "  Batch   440  of  3,755.    Elapsed: 0:42:34.\n",
      "  Batch   480  of  3,755.    Elapsed: 0:46:24.\n",
      "  Batch   520  of  3,755.    Elapsed: 0:50:20.\n",
      "  Batch   560  of  3,755.    Elapsed: 0:57:55.\n",
      "  Batch   600  of  3,755.    Elapsed: 1:01:44.\n",
      "  Batch   640  of  3,755.    Elapsed: 1:05:31.\n",
      "  Batch   680  of  3,755.    Elapsed: 1:09:18.\n",
      "  Batch   720  of  3,755.    Elapsed: 1:13:04.\n",
      "  Batch   760  of  3,755.    Elapsed: 1:16:13.\n",
      "  Batch   800  of  3,755.    Elapsed: 1:18:49.\n",
      "  Batch   840  of  3,755.    Elapsed: 1:21:37.\n",
      "  Batch   880  of  3,755.    Elapsed: 1:24:29.\n",
      "  Batch   920  of  3,755.    Elapsed: 1:27:21.\n",
      "  Batch   960  of  3,755.    Elapsed: 1:29:51.\n",
      "  Batch 1,000  of  3,755.    Elapsed: 1:32:13.\n",
      "  Batch 1,040  of  3,755.    Elapsed: 1:34:41.\n",
      "  Batch 1,080  of  3,755.    Elapsed: 1:37:08.\n",
      "  Batch 1,120  of  3,755.    Elapsed: 1:39:47.\n",
      "  Batch 1,160  of  3,755.    Elapsed: 1:42:15.\n",
      "  Batch 1,200  of  3,755.    Elapsed: 1:44:39.\n",
      "  Batch 1,240  of  3,755.    Elapsed: 1:47:06.\n",
      "  Batch 1,280  of  3,755.    Elapsed: 1:49:33.\n",
      "  Batch 1,320  of  3,755.    Elapsed: 1:51:57.\n",
      "  Batch 1,360  of  3,755.    Elapsed: 1:54:22.\n",
      "  Batch 1,400  of  3,755.    Elapsed: 1:56:47.\n",
      "  Batch 1,440  of  3,755.    Elapsed: 1:59:06.\n",
      "  Batch 1,480  of  3,755.    Elapsed: 2:01:23.\n",
      "  Batch 1,520  of  3,755.    Elapsed: 2:03:41.\n",
      "  Batch 1,560  of  3,755.    Elapsed: 2:05:57.\n",
      "  Batch 1,600  of  3,755.    Elapsed: 2:08:14.\n",
      "  Batch 1,640  of  3,755.    Elapsed: 2:10:39.\n",
      "  Batch 1,680  of  3,755.    Elapsed: 2:13:00.\n",
      "  Batch 1,720  of  3,755.    Elapsed: 2:15:21.\n",
      "  Batch 1,760  of  3,755.    Elapsed: 2:17:41.\n",
      "  Batch 1,800  of  3,755.    Elapsed: 2:20:01.\n",
      "  Batch 1,840  of  3,755.    Elapsed: 2:22:23.\n",
      "  Batch 1,880  of  3,755.    Elapsed: 2:24:43.\n",
      "  Batch 1,920  of  3,755.    Elapsed: 2:27:04.\n",
      "  Batch 1,960  of  3,755.    Elapsed: 2:29:21.\n",
      "  Batch 2,000  of  3,755.    Elapsed: 2:31:39.\n",
      "  Batch 2,040  of  3,755.    Elapsed: 2:33:56.\n",
      "  Batch 2,080  of  3,755.    Elapsed: 2:36:11.\n",
      "  Batch 2,120  of  3,755.    Elapsed: 2:38:28.\n",
      "  Batch 2,160  of  3,755.    Elapsed: 2:40:46.\n",
      "  Batch 2,200  of  3,755.    Elapsed: 2:43:05.\n",
      "  Batch 2,240  of  3,755.    Elapsed: 2:45:24.\n",
      "  Batch 2,280  of  3,755.    Elapsed: 2:47:40.\n",
      "  Batch 2,320  of  3,755.    Elapsed: 2:49:56.\n",
      "  Batch 2,360  of  3,755.    Elapsed: 2:52:12.\n",
      "  Batch 2,400  of  3,755.    Elapsed: 2:54:19.\n",
      "  Batch 2,440  of  3,755.    Elapsed: 2:56:53.\n",
      "  Batch 2,480  of  3,755.    Elapsed: 2:59:03.\n",
      "  Batch 2,520  of  3,755.    Elapsed: 3:01:16.\n",
      "  Batch 2,560  of  3,755.    Elapsed: 3:03:33.\n",
      "  Batch 2,600  of  3,755.    Elapsed: 3:05:53.\n",
      "  Batch 2,640  of  3,755.    Elapsed: 3:08:35.\n",
      "  Batch 2,680  of  3,755.    Elapsed: 3:11:27.\n",
      "  Batch 2,720  of  3,755.    Elapsed: 3:14:28.\n",
      "  Batch 2,760  of  3,755.    Elapsed: 3:17:30.\n",
      "  Batch 2,800  of  3,755.    Elapsed: 3:19:55.\n",
      "  Batch 2,840  of  3,755.    Elapsed: 3:22:10.\n",
      "  Batch 2,880  of  3,755.    Elapsed: 3:24:26.\n",
      "  Batch 2,920  of  3,755.    Elapsed: 3:26:43.\n",
      "  Batch 2,960  of  3,755.    Elapsed: 3:29:03.\n",
      "  Batch 3,000  of  3,755.    Elapsed: 3:31:24.\n",
      "  Batch 3,040  of  3,755.    Elapsed: 3:33:48.\n",
      "  Batch 3,080  of  3,755.    Elapsed: 3:36:15.\n",
      "  Batch 3,120  of  3,755.    Elapsed: 3:38:52.\n",
      "  Batch 3,160  of  3,755.    Elapsed: 3:42:03.\n",
      "  Batch 3,200  of  3,755.    Elapsed: 3:45:25.\n",
      "  Batch 3,240  of  3,755.    Elapsed: 3:48:47.\n",
      "  Batch 3,280  of  3,755.    Elapsed: 3:52:08.\n",
      "  Batch 3,320  of  3,755.    Elapsed: 3:55:31.\n",
      "  Batch 3,360  of  3,755.    Elapsed: 3:58:51.\n",
      "  Batch 3,400  of  3,755.    Elapsed: 4:02:11.\n",
      "  Batch 3,440  of  3,755.    Elapsed: 4:05:29.\n",
      "  Batch 3,480  of  3,755.    Elapsed: 4:08:47.\n",
      "  Batch 3,520  of  3,755.    Elapsed: 4:11:58.\n",
      "  Batch 3,560  of  3,755.    Elapsed: 4:15:06.\n",
      "  Batch 3,600  of  3,755.    Elapsed: 4:18:09.\n",
      "  Batch 3,640  of  3,755.    Elapsed: 4:21:15.\n",
      "  Batch 3,680  of  3,755.    Elapsed: 4:24:39.\n",
      "  Batch 3,720  of  3,755.    Elapsed: 4:28:00.\n",
      "\n",
      "  Average training loss: 0.68\n",
      "  Training epoch took: 4:30:53\n",
      "\n",
      "Running Validation...\n",
      "Exact match: 26.8, F1 score: 63.70522052170986\n",
      "\n",
      "  Validation took: 0:37:49\n",
      " \n",
      "=====Epoch 3=====\n",
      "Training....\n",
      "  Batch    40  of  3,755.    Elapsed: 0:03:31.\n",
      "  Batch    80  of  3,755.    Elapsed: 0:06:58.\n",
      "  Batch   120  of  3,755.    Elapsed: 0:10:23.\n",
      "  Batch   160  of  3,755.    Elapsed: 0:14:01.\n",
      "  Batch   200  of  3,755.    Elapsed: 0:16:35.\n",
      "  Batch   240  of  3,755.    Elapsed: 0:18:56.\n",
      "  Batch   280  of  3,755.    Elapsed: 0:21:20.\n",
      "  Batch   320  of  3,755.    Elapsed: 0:23:45.\n",
      "  Batch   360  of  3,755.    Elapsed: 0:26:11.\n",
      "  Batch   400  of  3,755.    Elapsed: 0:28:41.\n",
      "  Batch   440  of  3,755.    Elapsed: 0:31:09.\n",
      "  Batch   480  of  3,755.    Elapsed: 0:33:38.\n",
      "  Batch   520  of  3,755.    Elapsed: 0:36:10.\n",
      "  Batch   560  of  3,755.    Elapsed: 0:38:47.\n",
      "  Batch   600  of  3,755.    Elapsed: 0:41:17.\n",
      "  Batch   640  of  3,755.    Elapsed: 0:43:49.\n",
      "  Batch   680  of  3,755.    Elapsed: 0:46:23.\n",
      "  Batch   720  of  3,755.    Elapsed: 0:48:57.\n",
      "  Batch   760  of  3,755.    Elapsed: 0:51:34.\n",
      "  Batch   800  of  3,755.    Elapsed: 0:54:10.\n",
      "  Batch   840  of  3,755.    Elapsed: 0:56:48.\n",
      "  Batch   880  of  3,755.    Elapsed: 0:59:30.\n",
      "  Batch   920  of  3,755.    Elapsed: 1:02:11.\n",
      "  Batch   960  of  3,755.    Elapsed: 1:04:58.\n",
      "  Batch 1,000  of  3,755.    Elapsed: 1:07:37.\n",
      "  Batch 1,040  of  3,755.    Elapsed: 1:10:15.\n",
      "  Batch 1,080  of  3,755.    Elapsed: 1:12:51.\n",
      "  Batch 1,120  of  3,755.    Elapsed: 1:15:29.\n",
      "  Batch 1,160  of  3,755.    Elapsed: 1:18:06.\n",
      "  Batch 1,200  of  3,755.    Elapsed: 1:20:43.\n",
      "  Batch 1,240  of  3,755.    Elapsed: 1:23:19.\n",
      "  Batch 1,280  of  3,755.    Elapsed: 1:25:54.\n",
      "  Batch 1,320  of  3,755.    Elapsed: 1:28:31.\n",
      "  Batch 1,360  of  3,755.    Elapsed: 1:31:08.\n",
      "  Batch 1,400  of  3,755.    Elapsed: 1:33:38.\n",
      "  Batch 1,440  of  3,755.    Elapsed: 1:36:10.\n",
      "  Batch 1,480  of  3,755.    Elapsed: 1:38:41.\n",
      "  Batch 1,520  of  3,755.    Elapsed: 1:41:11.\n",
      "  Batch 1,560  of  3,755.    Elapsed: 1:43:43.\n",
      "  Batch 1,600  of  3,755.    Elapsed: 1:46:08.\n",
      "  Batch 1,640  of  3,755.    Elapsed: 1:48:30.\n",
      "  Batch 1,680  of  3,755.    Elapsed: 1:50:52.\n",
      "  Batch 1,720  of  3,755.    Elapsed: 1:53:14.\n",
      "  Batch 1,760  of  3,755.    Elapsed: 1:55:32.\n",
      "  Batch 1,800  of  3,755.    Elapsed: 1:57:50.\n",
      "  Batch 1,840  of  3,755.    Elapsed: 2:00:08.\n",
      "  Batch 1,880  of  3,755.    Elapsed: 2:02:27.\n",
      "  Batch 1,920  of  3,755.    Elapsed: 2:04:43.\n",
      "  Batch 1,960  of  3,755.    Elapsed: 2:07:05.\n",
      "  Batch 2,000  of  3,755.    Elapsed: 2:09:22.\n",
      "  Batch 2,040  of  3,755.    Elapsed: 2:11:41.\n",
      "  Batch 2,080  of  3,755.    Elapsed: 2:14:01.\n",
      "  Batch 2,120  of  3,755.    Elapsed: 2:16:22.\n",
      "  Batch 2,160  of  3,755.    Elapsed: 2:18:42.\n",
      "  Batch 2,200  of  3,755.    Elapsed: 2:21:03.\n",
      "  Batch 2,240  of  3,755.    Elapsed: 2:23:25.\n",
      "  Batch 2,280  of  3,755.    Elapsed: 2:25:47.\n",
      "  Batch 2,320  of  3,755.    Elapsed: 2:28:14.\n",
      "  Batch 2,360  of  3,755.    Elapsed: 2:30:39.\n",
      "  Batch 2,400  of  3,755.    Elapsed: 2:33:03.\n",
      "  Batch 2,440  of  3,755.    Elapsed: 2:35:28.\n",
      "  Batch 2,480  of  3,755.    Elapsed: 2:37:51.\n",
      "  Batch 2,520  of  3,755.    Elapsed: 2:40:14.\n",
      "  Batch 2,560  of  3,755.    Elapsed: 2:42:37.\n",
      "  Batch 2,600  of  3,755.    Elapsed: 2:45:01.\n",
      "  Batch 2,640  of  3,755.    Elapsed: 2:47:27.\n",
      "  Batch 2,680  of  3,755.    Elapsed: 2:49:54.\n",
      "  Batch 2,720  of  3,755.    Elapsed: 2:52:21.\n",
      "  Batch 2,760  of  3,755.    Elapsed: 2:54:46.\n",
      "  Batch 2,800  of  3,755.    Elapsed: 2:57:12.\n",
      "  Batch 2,840  of  3,755.    Elapsed: 2:59:41.\n",
      "  Batch 2,880  of  3,755.    Elapsed: 3:02:06.\n",
      "  Batch 2,920  of  3,755.    Elapsed: 3:04:30.\n",
      "  Batch 2,960  of  3,755.    Elapsed: 3:07:02.\n",
      "  Batch 3,000  of  3,755.    Elapsed: 3:09:28.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Batch 3,040  of  3,755.    Elapsed: 3:11:50.\n",
      "  Batch 3,080  of  3,755.    Elapsed: 3:14:17.\n",
      "  Batch 3,120  of  3,755.    Elapsed: 3:16:42.\n",
      "  Batch 3,160  of  3,755.    Elapsed: 3:19:01.\n",
      "  Batch 3,200  of  3,755.    Elapsed: 3:21:25.\n",
      "  Batch 3,240  of  3,755.    Elapsed: 3:23:50.\n",
      "  Batch 3,280  of  3,755.    Elapsed: 3:26:16.\n",
      "  Batch 3,320  of  3,755.    Elapsed: 3:28:48.\n",
      "  Batch 3,360  of  3,755.    Elapsed: 3:31:07.\n",
      "  Batch 3,400  of  3,755.    Elapsed: 3:33:25.\n",
      "  Batch 3,440  of  3,755.    Elapsed: 3:35:50.\n",
      "  Batch 3,480  of  3,755.    Elapsed: 3:38:14.\n",
      "  Batch 3,520  of  3,755.    Elapsed: 3:40:39.\n",
      "  Batch 3,560  of  3,755.    Elapsed: 3:43:04.\n",
      "  Batch 3,600  of  3,755.    Elapsed: 3:45:31.\n",
      "  Batch 3,640  of  3,755.    Elapsed: 3:47:57.\n",
      "  Batch 3,680  of  3,755.    Elapsed: 3:50:18.\n",
      "  Batch 3,720  of  3,755.    Elapsed: 3:52:36.\n",
      "\n",
      "  Average training loss: 0.38\n",
      "  Training epoch took: 3:54:35\n",
      "\n",
      "Running Validation...\n",
      "Exact match: 24.933333333333334, F1 score: 62.120821474024766\n",
      "\n",
      "  Validation took: 0:29:14\n",
      "\n",
      "Training complete!\n",
      "Total training took 21:56:51 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "import random,time\n",
    "import numpy as np\n",
    "\n",
    "# to reproduce results\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "\n",
    "#storing all training and validation stats\n",
    "stats = []\n",
    "\n",
    "\n",
    "#to measure total training time\n",
    "total_train_time_start = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(' ')\n",
    "    print(f'=====Epoch {epoch + 1}=====')\n",
    "    print('Training....')\n",
    "     \n",
    "    # ===============================\n",
    "    #    Train\n",
    "    # ===============================   \n",
    "    # measure how long training epoch takes\n",
    "    t0 = time.time()\n",
    "     \n",
    "    training_loss = 0\n",
    "    # loop through train data\n",
    "    model.train()\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "         \n",
    "        # we will print train time in every 40 epochs\n",
    "        if step%40 == 0 and not step == 0:\n",
    "              elapsed_time = format_time(time.time() - t0)\n",
    "              # Report progress.\n",
    "              print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed_time))\n",
    "\n",
    "         \n",
    "       \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        start_positions = batch['start_positions'].to(device)\n",
    "        end_positions = batch['end_positions'].to(device)\n",
    "            \n",
    "\n",
    "\n",
    "        #set gradients to zero\n",
    "        model.zero_grad()\n",
    "\n",
    "        result = model(input_ids = input_ids, \n",
    "                        attention_mask = attention_mask,\n",
    "                        start_positions = start_positions,\n",
    "                        end_positions = end_positions,\n",
    "                        return_dict=True)\n",
    "         \n",
    "        loss = result.loss\n",
    "    \n",
    "        #accumulate the loss over batches so that we can calculate avg loss at the end\n",
    "        training_loss += loss.item()      \n",
    "\n",
    "        #perform backward prorpogation\n",
    "        loss.backward()\n",
    "\n",
    "        # update the gradients\n",
    "        optimizer.step()\n",
    "\n",
    "    # calculate avg loss\n",
    "    avg_train_loss = training_loss/len(train_dataloader) \n",
    " \n",
    "    # calculates training time\n",
    "    training_time = format_time(time.time() - t0)\n",
    "     \n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epoch took: {:}\".format(training_time))\n",
    "    \n",
    "    \n",
    "    # ===============================\n",
    "    #    Validation\n",
    "    # ===============================\n",
    "     \n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Put the model in evaluation mode--the dropout layers behave differently\n",
    "    # during evaluation.\n",
    "    model.eval()\n",
    "     \n",
    "\n",
    "    start_logits,end_logits = [],[]\n",
    "    for step,batch in enumerate(eval_dataloader):\n",
    "         \n",
    "       \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "         \n",
    "        # Tell pytorch not to bother with constructing the compute graph during\n",
    "        # the forward pass, since this is only needed for backprop (training).\n",
    "        with torch.no_grad():  \n",
    "             result = model(input_ids = input_ids, \n",
    "                        attention_mask = attention_mask,return_dict=True)\n",
    "        \n",
    "\n",
    "\n",
    "        start_logits.append(result.start_logits.cpu().numpy())\n",
    "        end_logits.append(result.end_logits.cpu().numpy())\n",
    "   \n",
    "\n",
    "    start_logits = np.concatenate(start_logits)\n",
    "    end_logits = np.concatenate(end_logits)\n",
    "    # start_logits = start_logits[: len(val_dataset)]\n",
    "    # end_logits = end_logits[: len(val_dataset)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # calculating metrics\n",
    "    answers,metrics_ = predict_answers_and_evaluate(start_logits,end_logits,validation_processed_dataset,dataset[\"validation\"])\n",
    "    print(f'Exact match: {metrics_[\"exact_match\"]}, F1 score: {metrics_[\"f1\"]}')\n",
    "\n",
    "\n",
    "    print('')\n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_train_time_start)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d69e6669",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have about 70 tokens generated\n",
      " \n",
      "Some examples of token-input_id pairs:\n",
      "[CLS] : 101\n",
      "how : 2129\n",
      "many : 2116\n",
      "parameters : 11709\n",
      "does : 2515\n",
      "bert : 14324\n",
      "- : 1011\n",
      "large : 2312\n",
      "have : 2031\n",
      "? : 1029\n",
      "[SEP] : 102\n",
      "bert : 14324\n",
      "- : 1011\n",
      "large : 2312\n",
      "is : 2003\n",
      "really : 2428\n",
      "big : 2502\n",
      ". : 1012\n",
      ". : 1012\n",
      ". : 1012\n",
      "it : 2009\n",
      "has : 2038\n",
      "24 : 2484\n",
      "- : 1011\n",
      "layers : 9014\n",
      "and : 1998\n",
      "an : 2019\n",
      "em : 7861\n",
      "##bed : 8270\n",
      "##ding : 4667\n",
      "size : 2946\n",
      "of : 1997\n",
      "1 : 1015\n",
      ", : 1010\n",
      "02 : 6185\n",
      "##4 : 2549\n",
      ", : 1010\n",
      "for : 2005\n",
      "a : 1037\n",
      "total : 2561\n",
      "of : 1997\n",
      "340 : 16029\n",
      "##m : 2213\n",
      "parameters : 11709\n",
      "! : 999\n",
      "altogether : 10462\n",
      "it : 2009\n",
      "is : 2003\n",
      "1 : 1015\n",
      ". : 1012\n",
      "34 : 4090\n",
      "##gb : 18259\n",
      ", : 1010\n",
      "so : 2061\n",
      "expect : 5987\n",
      "it : 2009\n",
      "to : 2000\n",
      "take : 2202\n",
      "a : 1037\n",
      "couple : 3232\n",
      "minutes : 2781\n",
      "to : 2000\n",
      "download : 8816\n",
      "to : 2000\n",
      "your : 2115\n",
      "cola : 15270\n",
      "##b : 2497\n",
      "instance : 6013\n",
      ". : 1012\n",
      "[SEP] : 102\n"
     ]
    }
   ],
   "source": [
    "question = \"How many parameters does BERT-large have?\"\n",
    "context = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"\n",
    "\n",
    "input_ids = tokenizer.encode(question, context)\n",
    "print (f'We have about {len(input_ids)} tokens generated')\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "print(\" \")\n",
    "print('Some examples of token-input_id pairs:')\n",
    "\n",
    "for i, (token,inp_id) in enumerate(zip(tokens,input_ids)):\n",
    "    print(token,\":\",inp_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2999eb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "sep_idx = tokens.index('[SEP]')\n",
    "\n",
    "# we will provide including [SEP] token which seperates question from context and 1 for rest.\n",
    "token_type_ids = [0 for i in range(sep_idx+1)] + [1 for i in range(sep_idx+1,len(tokens))]\n",
    "print(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4e49368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted answer: 340##m\n"
     ]
    }
   ],
   "source": [
    "# Run our example through the model.\n",
    "out = model(torch.tensor([input_ids]))\n",
    "\n",
    "start_logits,end_logits = out['start_logits'],out['end_logits']\n",
    "# Find the tokens with the highest `start` and `end` scores.\n",
    "answer_start = torch.argmax(start_logits)\n",
    "answer_end = torch.argmax(end_logits)\n",
    "\n",
    "ans = ''.join(tokens[answer_start:answer_end])\n",
    "print('Predicted answer:', ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993b5095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
